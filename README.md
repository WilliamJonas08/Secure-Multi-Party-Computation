# Secure-MultiParty-Computation

# Pr√©sentation projet
===================================================

Ce projet a √©t√© r√©alis√© dans le cadre d'un projet de mon master Intelligence Artificielle et Apprentissage Automatique.

Criteo est une entreprise fran√ßaise de reciblage publicitaire sur internet au chiffre d'affaires de 150 milliards d'euros, diffusant plus de 4 milliards d'annonces chaque jour.

L'objectif du projet (orient√© recherche) est d‚Äô√©valuer la pertinence, pour le domaine du Machine Learning, de la m√©thode cryptographique appel√©e Secure Multiparty Computation (sMPC) dans le cadre d‚Äôune m√©thode de F√©d√©rative Learning sur un algorithme de bandits.

## Les principales interrogations sont:
- Cette m√©thode permet elle d‚Äôaugmenter la pr√©cision des mod√®les ?
- Cette m√©thode consomme elle moins d‚Äô√©nergie ? (Donn√©es non export√©es sur le cloud)
- Quelles sont les limites de cette m√©thode ?

Nous nous appuyons sur la librairie open-source Crypten. Il s'agit d'un framework construit sur PyTorch pour faciliter la recherche en apprentissage automatique s√©curis√© et pr√©servant la vie priv√©e. Crypten met en ≈ìuvre la m√©thode sMPC, qui crypte l'information en divisant les donn√©es entre plusieurs parties, qui peuvent chacune effectuer des calculs sur leur part mais ne sont pas capables de lire les donn√©es originales.

## üìå La strat√©gie utilis√©e consiste √† confronter les 3 mod√®les suivants :
- Algorithme de Bandits ‚Äòclassique‚Äô
- Algorithme de Bandits entrain√© via Federated Learning
- Algorithme de Bandits entrain√© via Federated Learning avec m√©thode sMPC


QLearningMouse
===================================================

<b>QLearningMouse</b>  is a small cat-mouse-cheese game based on [Q-Learning](https://en.wikipedia.org/wiki/Q-learning). The original version is by [vmayoral](https://github.com/vmayoral): [basic_reinforcement_learning:tutorial1](https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial1), I reconstructed his code to make the game more configurable, and what different most is that I use breadth-first-search([BFS](https://en.wikipedia.org/wiki/Breadth-first_search)) when cat chasing the AI mouse, so the cat looks much more brutal :P 

## About the game
Cat always chase the mouse in the shortest path, however the mouse first does not know the danger of being eaten. 
* <b>Mouse win</b> when eating the cheese and earns rewards value of 50, then a new cheese will be produced in a random grid.
* <b>cat win</b> when eating the mouse, the latter will gain rewards value of -100 when dead. Then it will relive in a random grid.

## Algorithm  
The basic algorithm of Q-Learning is:  
```
Q(s, a) += alpha * (reward(s,a) + gamma * max(Q(s', a') - Q(s,a))
#UPDATE TO CONTEXTUAL BANDITS
```
    
```alpha``` is the learning rate.
```gamma``` is the value of the future reward.
It use the best next choice of utility in later state to update the former state. 

Learn more about Q-Learning:  
1. [The Markov Decision Problem : Value Iteration and Policy Iteration](http://ais.informatik.uni-freiburg.de/teaching/ss03/ams/DecisionProblems.pdf)  
2. [ARTIFICIAL INTELLIGENCE FOUNDATIONS OF COMPUTATIONAL AGENTS : 11.3.3 Q-learning](http://artint.info/html/ArtInt_265.html)


## Example
Below we present a *mouse player* after **300 generations** of reinforcement learning:  
* blue is for mouse.
* black is for cat.
* orange is for cheese.

![](resources/snapshot1.gif)

After **339300 generations**:  

![](resources/snapshot2.gif)


## Reproduce it yourself

```bash
git clone https://github.com/fancoo/QLearningMouse
cd QLearningMouse
python greedyMouse.py
```


## Maj fed learning
pour update : oblig√© de :
- it√©rer sur les mondes
- it√©rer sur chacun de leur agents
- maj selon l'update commune √† r√©aliser

Algo de bandits correspond aux algo de TD Learning ? (savoir si on doit adapter le code et g√©n√©rer des contextes)


Separation taches : 
- class Worlds
- algo bandit/TD Learning
- mise en place/ g√©n√©ration contextes
